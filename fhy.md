## 神马面试

### Episode I:

#### 问题1: 详细描述下已有的工作，例如文本摘要的生成过程，段文本匹配的方法等。
回答：先描述总体方法思路，面试官应该会故意装不懂，问一些细节的东西，例如使用何种特征，如何training，如何testing。

#### 问题2: 详细描述LDA模型，包含背后的假设和数学原理。
回答：先从生成文档的角度描述LDA，例如每个文档d是关于所有主题z的分布，而每个主题z是所有单词的一个分布。然后从反向角度描述有了观测到的文档和单词，如何去反推d的主题分布和z的单词分布。相关知识链接：[LDA的数学原理](http://www.52nlp.cn/author/rickjin)

#### 问题3: 概率题，甲乙两人掷硬币（无偏），先出现10次正面则甲获胜，先出现10次反面则乙获胜。现在已经出现了8次正面，7次反面。问接下来甲乙获胜的概率各为多少？
回答：只需要算甲获胜的概率即可。按照还需要2、3、4次获胜的情况讨论，然后相加。

### Episode II:

#### 问题1: 现有海量的文本搜索数据，query和对应的document，需要使用deep learning来训练一个计算query与document相关性的模型。第一，如何设计这个模型；第二，针对大型的CPU集群，使用server－worker架构时，如何在worker本地进行优化来应对昂贵的网络通信资源瓶颈。
回答：首先需要使用一些embedding的方法来把变长的query和document转化为定长的特征向量（可以使用word2vec或者RNN等方法），在定长特征的基础上，再使用几个全联接层来学习中层特征，output层则可以使用相似度函数来作为loss函数（例如cosine距离等）。每个worker都会接受到少量的训练样本，如果每次迭代都要将计算得到的梯度返回给server来更新weights则会耗费大量的通信资源。所以可在worker本地保存一份weights数据，每次迭代后在worker本地先更新weights，多次迭代更新后再将数据统一返回给server来进行更新，这样做对于worker来说可能会造成对少量数据的overfitting，但是可以通过多次随机重新分配训练数据来使得每个worker在大的时间尺度上使用了大量的不同训练数据。

#### 问题2: 短文本匹配中，如何计算每个单词的权重？对于A、B两个单词，如何计算其转义概率（即AB之间插入其他文本时，含义转变的概率大小）？
回答：从IDF，词性，是否是entity等方面考虑构造特征。第二个方面不知道正确答案……